# Triton HRM Optimization Project
***Note:- This is version 1 a simple foundational implementation for learning purposes. The next update will deliver a more polished, optimized, and plug-in-ready project that can be used in real workflows.***

## Note:- We need very specific dependency versions to work with Triton.
## You need cuda ( NVIDIA GPU ) and linux enviroment to work with Triton. For Windows users try to use with WSL2.
- Make sure WSL2 is enabled and you have Ubuntu 22.04 installed.
- NVIDIA driver with WSL support (normal game read drivers already include this).

This project provides a modular Triton kernel library and HRM specific optimizations to accelerate the Hierarchical Reasoning Model (HRM) by reducing sequential bottlenecks.

It includes:

Core Triton kernels (MatMul, Softmax, Dropout, Attention, etc.)

HRM-optimized kernels (loop unrolling, head-parallelism, speculative chunking, fused forward+backward)

Advanced integrations (CUDA Graphs, Mixed Precision + Block Scaling, Pipeline Parallelism, Checkpointing, Multi-GPU heads)

A full benchmark suite comparing naive PyTorch HRM vs Triton-accelerated HRM.

T_HRM_OPT_ADV/
â”‚â”€â”€ README.md
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ utils.py
â”‚
â”œâ”€â”€ kernels/                     # Core + HRM-specific Triton kernels
â”‚       kernels_t/               # Core Triton kernels
â”‚           â”œâ”€â”€ vector_add.py
â”‚           â”œâ”€â”€ matmul.py
â”‚           â”œâ”€â”€ group_gemm.py
â”‚           â”œâ”€â”€ block_scaled_matmul.py
â”‚           â”œâ”€â”€ layernorm.py
â”‚           â”œâ”€â”€ softmax.py
â”‚           â”œâ”€â”€ dropout.py
â”‚           â”œâ”€â”€ attention.py
â”‚           â”œâ”€â”€ libdevice_ops.py
â”‚       kernels_h/                # Core HRM-specific Triton kernels
â”‚           â”œâ”€â”€ hrm_unrolled.py
â”‚           â”œâ”€â”€ hrm_heads.py
â”‚           â”œâ”€â”€ hrm_chunks.py
â”‚           â”œâ”€â”€ hrm_fused_fwbw.py
â”‚           â”œâ”€â”€ hrm_cuda_graphs.py
â”‚           â”œâ”€â”€ hrm_mixed_block.py
â”‚           â”œâ”€â”€ hrm_pipeline.py
â”‚           â”œâ”€â”€ hrm_checkpoint.py
â”‚           â”œâ”€â”€ hrm_multi_gpu_heads.py
â”œâ”€â”€ benchmarks/
â”‚   â”œâ”€â”€ bench_hrm.py          # Benchmark suite
â”‚
â”œâ”€â”€ training_hrm_training.py      # Example training loop with something closer to HRM block design


## Baseline Kernels

- MatMul & Group GEMM
- Block-Scaled MatMul
- LayerNorm / RMSNorm
- Fused Softmax
- Low-Memory Dropout
- Fused Attention
- Vector Add (utility)
- Libdevice Ops (asin)

## HRM-Specific Optimizations
- Loop Unrolling (tl.static_range)
- Head-Parallelism (FlashRNN-style block-diagonal heads)
- Speculative Chunk Execution (parallel chunks)
- Fused Forward + Backward (custom Triton autograd)

## Advanced Integrations
- CUDA Graphs (remove Python overhead)
- Mixed Precision + Block Scaling
- Pipeline Parallelism (GPipe-style across GPUs)
- Checkpointing + Custom Backprop
- Multi-GPU Head Parallelism

Clone the repo and install dependencies: You can do it by yourself 
Running the benchmark and HRM training : If you trust this work please run it at your own risk. ðŸ«¡

## The benchmarks/bench_hrm.py script compares:

- Naive PyTorch HRM
- Triton Unrolled HRM
- Head-Parallel HRM
- Pipeline HRM
- CUDA Graph HRM
- Multi-GPU HRM (if available)

Metrics:

- Step Time (ms)
- VRAM Usage (MB)
- Tokens/sec (extendable)

## Next Steps

Extend kernels to integrate FlashAttention2 primitives

Explore HRM decoding

Add distributed benchmarks (multi node HRM)


# Built with by an confused guy ðŸ˜•